# Koguma-LM Model Configuration

model:
  name: "koguma-350m"
  vocab_size: 50000  # SentencePiece vocab size
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  max_position_embeddings: 32768  # Long context support
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  layer_norm_eps: 1e-5
  initializer_range: 0.02
  use_cache: true
  
  # Architecture optimizations
  use_flash_attention: true
  use_rotary_embeddings: true  # RoPE for long context
  use_gated_mlp: true  # SwiGLU activation

tokenizer:
  type: "sentencepiece"
  model_path: "models/tokenizer.model"
  vocab_size: 50000
  model_type: "unigram"
  character_coverage: 0.9995
  
  # Japanese specific settings
  normalization_rule_name: "nfkc"
  add_dummy_prefix: true
  remove_extra_whitespaces: true

teacher_models:
  - name: "cyberagent/calm2-7b"
    weight: 0.3
    specialization: "japanese"
  - name: "codellama/CodeLlama-7b-hf"
    weight: 0.2
    specialization: "code"
  - name: "mistralai/Mistral-7B-v0.1"
    weight: 0.3
    specialization: "general"
  - name: "deepseek-ai/deepseek-math-7b-base"
    weight: 0.2
    specialization: "math"